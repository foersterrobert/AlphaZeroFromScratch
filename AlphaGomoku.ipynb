{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from modules.resnet import Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797c3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055060114711523056\n",
      "[[ 0.  0. -1.  0. -1.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "tensor([[[[0., 0., 1., 0., 1.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0., 1., 0.],\n",
      "          [1., 0., 1., 0., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gomoku = Gomoku()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = gomoku.get_initial_state()\n",
    "state = gomoku.get_next_state(state, 2, -1)\n",
    "state = gomoku.get_next_state(state, 4, -1)\n",
    "state = gomoku.get_next_state(state, 6, 1)\n",
    "state = gomoku.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = gomoku.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(gomoku, 4, 64, device=device)\n",
    "# model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(gomoku.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e997f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                    \n",
    "                else:\n",
    "                    spg.node = node\n",
    "                    \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "                    \n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "                \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0a5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "        \n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "            \n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs = temperature_action_probs / temperature_action_probs.sum(0)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "                    \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e2759c73da489da1da00afb7b1b917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_searches\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m60\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdirichlet_alpha\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.3\u001b[39m\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     22\u001b[0m alphaZero \u001b[39m=\u001b[39m AlphaZeroParallel(model, optimizer, game, args)\n\u001b[0;32m---> 23\u001b[0m alphaZero\u001b[39m.\u001b[39;49mlearn()\n",
      "Cell \u001b[0;32mIn[10], line 80\u001b[0m, in \u001b[0;36mAlphaZeroParallel.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m selfPlay_iteration \u001b[39min\u001b[39;00m trange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_selfPlay_iterations\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_parallel_games\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m---> 80\u001b[0m     memory \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselfPlay()\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m trange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mAlphaZeroParallel.selfPlay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([spg\u001b[39m.\u001b[39mstate \u001b[39mfor\u001b[39;00m spg \u001b[39min\u001b[39;00m spGames])\n\u001b[1;32m     16\u001b[0m neutral_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mchange_perspective(states, player)\n\u001b[0;32m---> 18\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmcts\u001b[39m.\u001b[39;49msearch(neutral_states, spGames)\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(spGames))[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     spg \u001b[39m=\u001b[39m spGames[i]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mMCTSParallel.search\u001b[0;34m(self, states, spGames)\u001b[0m\n\u001b[1;32m     28\u001b[0m node \u001b[39m=\u001b[39m spg\u001b[39m.\u001b[39mroot\n\u001b[1;32m     30\u001b[0m \u001b[39mwhile\u001b[39;00m node\u001b[39m.\u001b[39mis_fully_expanded():\n\u001b[0;32m---> 31\u001b[0m     node \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mselect()\n\u001b[1;32m     33\u001b[0m value, is_terminal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_value_and_terminated(node\u001b[39m.\u001b[39mstate, node\u001b[39m.\u001b[39maction_taken)\n\u001b[1;32m     34\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_opponent_value(value)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mNode.select\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m best_ucb \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren:\n\u001b[0;32m---> 23\u001b[0m     ucb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_ucb(child)\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m ucb \u001b[39m>\u001b[39m best_ucb:\n\u001b[1;32m     25\u001b[0m         best_child \u001b[39m=\u001b[39m child\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = Gomoku()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(gomoku, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = ConnectFour()\n",
    "# player = 1\n",
    "\n",
    "# args = {\n",
    "#     'C': 2,\n",
    "#     'num_searches': 600,\n",
    "#     'dirichlet_epsilon': 0.,\n",
    "#     'dirichlet_alpha': 0.3\n",
    "# }\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = ResNet(game, 9, 128, device)\n",
    "# model.load_state_dict(torch.load(\"model_7_ConnectFour.pt\", map_location=device))\n",
    "# model.eval()\n",
    "\n",
    "# mcts = MCTS(game, args, model)\n",
    "\n",
    "# state = game.get_initial_state()\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     print(state)\n",
    "    \n",
    "#     if player == 1:\n",
    "#         valid_moves = game.get_valid_moves(state)\n",
    "#         print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "#         action = int(input(f\"{player}:\"))\n",
    "\n",
    "#         if valid_moves[action] == 0:\n",
    "#             print(\"action not valid\")\n",
    "#             continue\n",
    "            \n",
    "#     else:\n",
    "#         neutral_state = game.change_perspective(state, player)\n",
    "#         mcts_probs = mcts.search(neutral_state)\n",
    "#         action = np.argmax(mcts_probs)\n",
    "        \n",
    "#     state = game.get_next_state(state, action, player)\n",
    "    \n",
    "#     value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "#     if is_terminal:\n",
    "#         print(state)\n",
    "#         if value == 1:\n",
    "#             print(player, \"won\")\n",
    "#         else:\n",
    "#             print(\"draw\")\n",
    "#         break\n",
    "        \n",
    "#     player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c470145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 6, 7, 8]\n",
      "action not valid\n",
      "[[ 0.  0.  0.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 6, 7, 8]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0. -1.]]\n",
      "valid_moves [0, 2, 3, 6, 7]\n",
      "action not valid\n",
      "[[ 0.  1.  0.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0. -1.]]\n",
      "valid_moves [0, 2, 3, 6, 7]\n",
      "[[ 1.  1.  0.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0. -1.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  1.]\n",
      " [ 0.  0. -1.]]\n",
      "valid_moves [3, 6, 7]\n",
      "[[ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 0.  0. -1.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [-1.  0. -1.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device)\n",
    "model.load_state_dict(torch.load(f\"model_2_{game}.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
